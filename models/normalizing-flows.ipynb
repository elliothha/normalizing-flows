{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e5895f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import copy\n",
    "\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt  \n",
    "%matplotlib inline\n",
    "\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f59f7c",
   "metadata": {},
   "source": [
    "## Hyperparameters & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9c1a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS\n",
    "lr = 1e-4\n",
    "batch_size = 128\n",
    "num_epochs = 1000\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d734a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((8, 8)),  # resizing the images to 8x8 for computational complexity\n",
    "    transforms.ToTensor(),  # Renormalizes to [0,1] range\n",
    "    transforms.Lambda(lambda x: torch.flatten(x))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a4f1c3",
   "metadata": {},
   "source": [
    "## Helper Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b27f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLinear(nn.Linear):\n",
    "    '''\n",
    "    - purpose is a linear layer with masking between weights to enforce autoregressive property\n",
    "        - the mask is a lower triangular matrix with 0's on the diagonal\n",
    "        - matrix = W_{ij}, where [i][j] represents the path from input unit j -> output unit i\n",
    "        - so to enforce autoregressive property, W_{ij} = {1 if j < i, 0 otherwise}, i.e., lower tril\n",
    "    - inherits from nn.Linear to achieve this\n",
    "    '''\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__(in_features, out_features, bias)\n",
    "        # diagonal = -1 makes it a lower tril with 0's on the diagonal\n",
    "        self.mask = torch.tril(torch.ones(out_features, in_features), diagonal=-1).to(device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        masked_weights = self.weight * self.mask\n",
    "        return F.linear(x, masked_weights, self.bias)\n",
    "\n",
    "class MADE(nn.Module):\n",
    "    '''\n",
    "    - for MAF, an autoregressive layer (AR) = MADE, and they also use MADE to compute f_\\alpha, f_\\mu\n",
    "    - the two key points for this implementation are handling\n",
    "        - order-agnostic training\n",
    "            - MADE samples an ordering before each minibatch update for each layer\n",
    "            - MAF has 5 AR layers implementing MADE, and\n",
    "                - keeps the natural ordering of the input for the first AR layer\n",
    "                - reverses the ordering of the output after each AR layer as done in IAF\n",
    "        - connectivity-agnostic training\n",
    "            - MADE samples the degree of each unit for each layer before each minibatch update\n",
    "            - As far as I can tell, MAF just has it as a lower tril with the diag = 0\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            MaskedLinear(in_features=input_dim, out_features=hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            MaskedLinear(in_features=hidden_dim, out_features=output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x) # [BS, output_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37952f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiagonalScalingLayer(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.s = nn.Parameter(torch.zeros(dim, device=device)) # [D,]\n",
    "        \n",
    "    def forward(self, z):\n",
    "        ''' f: Z -> X\n",
    "        x = exp(S) * z\n",
    "        det(S) = \\Pi_i exp(S_{ii})\n",
    "        log|det(S)| = \\Sigma_i S_{ii}\n",
    "        '''\n",
    "        z = z * torch.exp(self.s) # [BS, D] * [D], use element-wise mult w diag vector\n",
    "        log_det = self.s.sum() \n",
    "        return z, log_det\n",
    "    \n",
    "    def inverse(self, x):\n",
    "        ''' f: X -> Z\n",
    "        z = x * 1/exp(S) = x * exp(S)^-1\n",
    "        det(S^-1) = \\Pi_i exp(S)^-1\n",
    "        log|det(S)| = -\\Sigma_i S_{ii}\n",
    "        '''\n",
    "        # For the inverse transformation, divide by the scales\n",
    "        x = x / torch.exp(self.s)\n",
    "        log_det = -self.s.sum()\n",
    "        return x, log_det"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3eca6e",
   "metadata": {},
   "source": [
    "## Autoregressive + Coupling Norm Flow Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e001c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAF(nn.Module):\n",
    "    '''\n",
    "    - THIS IS A MODULE DEFINING ONE TRANSFORMATION\n",
    "    - For MNIST, MAF stacks 5 transformations with a BatchNorm and order reversal after each one\n",
    "    - forward(self, z): SAMPLING, f: Z -> X\n",
    "        - I'm defining forward to be the pass through the Norm Flow model that maps f: Z -> X\n",
    "        - for MAF, this is transforming z -> x done sequentially as alpha and mu depend on x\n",
    "        - returns log_det here in order to compute the density of the generated sample x under p_X\n",
    "    - inverse(self, x): TRAINING, f: X -> Z\n",
    "        - I'm defining inverse to be the pass through the Norm Flow model that maps f: X -> Z\n",
    "        - for MAF, this means transforming external or internal x -> z\n",
    "        - z = f^-1(x) represents the inverted bijection function mapping the real data, x, to the\n",
    "        z that generated it. Use change of vars formula to express learned dist. p_X in terms of\n",
    "        this transformation, and get p_X ~ p_data by MLE under the real dataset\n",
    "    '''\n",
    "    def __init__(self, dim, hidden_dim, reverse_order=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.reverse_order = reverse_order\n",
    "        \n",
    "        # instead of having separate nets for alpha and mu,\n",
    "        # Kaparthy has one net that outputs a 2*input_dim tensor and splits it in half for them\n",
    "        self.f_params = MADE(dim, hidden_dim, dim*2) \n",
    "    \n",
    "    def forward(self, z):\n",
    "        ''' f: Z -> X\n",
    "        - for MAF, sampling is slow since each alpha_i and mu_i depend on x_i\n",
    "        - thus, done sequentially and need to go pixel by pixel\n",
    "        - FORWARD EQUATION: x_i = z_i * exp(alpha_i) + mu_i\n",
    "        - LOG DET OF f: \\Sigma_{i} \\alpha_{i}\n",
    "        - RETURNS: x (where x = f(z)) and log_det of f\n",
    "        '''\n",
    "        x = torch.zeros_like(z, device=device)\n",
    "        log_det = torch.zeros(z.size(0), device=device) # [BS,]\n",
    "        \n",
    "        for i in range(self.dim):\n",
    "            params = self.f_params(x)\n",
    "            mu, alpha = torch.chunk(params, 2, dim=1)\n",
    "            \n",
    "            x[:, i] = z[:, i] * torch.exp(alpha[:, i]) + mu[:, i] # sequential buildup\n",
    "            log_det += alpha[:, i] # [BS,]\n",
    "        \n",
    "        x = x.flip(dims=(1,)) if self.reverse_order else x # flip order after every AR layer\n",
    "        \n",
    "        return x, log_det # [BS, D], [BS,]\n",
    "    \n",
    "    def inverse(self, x):\n",
    "        ''' f^-1: X -> Z\n",
    "        - for MAF, alpha and mu depend on x, and the autoregressive property is enforced by MADE\n",
    "        - thus, we can vectorize our computation of p_X for fast training done in parallel\n",
    "        - this is done since we have all x, parallel-y computed mu and alpha, so can compute all of z\n",
    "        - INVERSE EQUATION: z_i = (x_i - mu_i) * exp(-alpha_i) -> z = (x - mu) * exp(-alpha)\n",
    "        - LOG DET OF f^-1: -(\\Sigma_{i} \\alpha_{i})\n",
    "        - RETURNS: z (where z = f^-1(x)) and log_det of f^-1\n",
    "        '''  \n",
    "        x = x.flip(dims=(1,)) if self.reverse_order else x\n",
    "        \n",
    "        params = self.f_params(x)\n",
    "        mu, alpha = torch.chunk(params, 2, dim=1)\n",
    "        \n",
    "        z = (x - mu) * torch.exp(-alpha)\n",
    "        log_det = -torch.sum(alpha, dim=1) # [BS,]\n",
    "        \n",
    "        return z, log_det # [BS, D], [BS,]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8461f80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IAF(nn.Module):\n",
    "    '''\n",
    "    - THIS IS A MODULE DEFINING ONE TRANSFORMATION OF IAF\n",
    "    - Core difference between IAF and MAF is that alpha and mu depend on z instead of x now\n",
    "    '''\n",
    "    def __init__(self, dim, hidden_dim, reverse_order=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.reverse_order = reverse_order\n",
    "        \n",
    "        self.f_params = MADE(dim, hidden_dim, dim*2) \n",
    "    \n",
    "    def forward(self, z):\n",
    "        ''' f: Z -> X\n",
    "        - for IAF, sampling is fast since alpha and mu depend on z now, which we have\n",
    "        - thus, we can compute samples quickly in parallel\n",
    "        - FORWARD EQUATION: x = z * exp(alpha) + mu\n",
    "        - LOG DET OF f: \\Sigma_{i} \\alpha_{i}\n",
    "        - RETURNS: x (where x = f(z)) and log_det of f\n",
    "        '''\n",
    "        params = self.f_params(z)\n",
    "        mu, alpha = torch.chunk(params, 2, dim=1)\n",
    "        \n",
    "        x = z * torch.exp(alpha) + mu\n",
    "        log_det = torch.sum(alpha, dim=1)\n",
    "        \n",
    "        x = x.flip(dims=(1,)) if self.reverse_order else x # flip after transforming in fpass\n",
    "        \n",
    "        return x, log_det # [BS, D], [BS, 1]\n",
    "    \n",
    "    def inverse(self, x):\n",
    "        ''' f: X -> Z\n",
    "        - for IAF, alpha and mu depend on z, and the autoregressive property is enforced by MADE\n",
    "        - thus, computing p_X is slow now since we need to recover z sequentially\n",
    "        - INVERSE EQUATION: z_i = (x_i - mu_i) * exp(-alpha_i)\n",
    "        - LOG DET OF f^-1: -(\\Sigma_{i} \\alpha_{i})\n",
    "        - RETURNS: z (where z = f^-1(x)) and log_det of f^-1\n",
    "        '''  \n",
    "        x = x.flip(dims=(1,)) if self.reverse_order else x\n",
    "        \n",
    "        z = torch.zeros_like(x, device=device)\n",
    "        log_det = torch.zeros(z.size(0), device=device) # [BS,]\n",
    "        \n",
    "        for i in range(self.dim):\n",
    "            params = self.f_params(z)\n",
    "            mu, alpha = torch.chunk(params, 2, dim=1)  \n",
    "            \n",
    "            z[:, i] = (x[:, i] - mu[:, i]) * torch.exp(-alpha[:, i])\n",
    "            log_det += -alpha[:, i] # [BS,]\n",
    "        \n",
    "        return z, log_det # [BS, D], [BS,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421cab0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NICE(nn.Module):\n",
    "    '''\n",
    "    - this is just for the coupling layer implementation, diagonal scaling matrix implemented later\n",
    "    - alternate partitioning handled in upper vs lower halves via reverse_order, not evens vs odds\n",
    "    '''\n",
    "    def __init__(self, dim, hidden_dim, reverse_order=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d = dim // 2\n",
    "        self.reverse_order = reverse_order\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(self.d, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, self.d)\n",
    "        )\n",
    "        \n",
    "    def forward(self, z):\n",
    "        ''' f: Z -> X\n",
    "        - log det of forward pass is actually equal to 0 since \n",
    "            - J = [[I, 0], [df/dz1, I]], identities on the diagonal and 0 on the opposite diagonal\n",
    "            - det(J) = 1\n",
    "            - log(det(J)) = log(1) = 0\n",
    "        '''\n",
    "        z1, z2 = z[:, :self.d], z[:, self.d:] # z1 = z1:d, z2 = zd+1:D\n",
    "        \n",
    "        x1, x2 = z1, z2 + self.net(z1) # x1 = unchanged z1, x2 = additive coupling w transformed z1\n",
    "        x = torch.cat((x1, x2), dim=1) # [BS, D]\n",
    "        \n",
    "        x = x.flip(dims=(1,)) if self.reverse_order else x\n",
    "        \n",
    "        log_det = torch.zeros(x.size(0), device=device) # [BS,]\n",
    "        return x, log_det # [BS, D], [BS,]\n",
    "        \n",
    "    def inverse(self, x):\n",
    "        ''' f: X -> Z\n",
    "        - inv log det is also 0\n",
    "        '''\n",
    "        x = x.flip(dims=(1,)) if self.reverse_order else x\n",
    "        \n",
    "        x1, x2 = x[:, :self.d], x[:, self.d:] # x1 = x1:d, x2 = xd+1:D\n",
    "        \n",
    "        z1 = x1 # z1 = unchanged x1\n",
    "        z2 = x2 - self.net(z1) # z2 = depends on z1, additive coupling w x2 and transformed z1\n",
    "        z = torch.cat((z1, z2), dim=1) # [BS, D]\n",
    "        \n",
    "        log_det = torch.zeros(x.size(0), device=device) # [BS,]\n",
    "        return z, log_det # [BS, D], [BS,]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8005e6e",
   "metadata": {},
   "source": [
    "## Normalizing Flow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f60c72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormFlow(nn.Module):\n",
    "    '''\n",
    "    - For MNIST\n",
    "        - MAF stacks 5 transformations with a BatchNorm and order reversal after each one\n",
    "        - NICE uses 4 coupling layers with no BatchNorm and evens/odds, but I'm just keeping \n",
    "        the MAF params (5 layers, BatchNorm, order reversal) and adding the diag scaling layer\n",
    "    - To get final log_det, just add up the log_dets from each transformation layer\n",
    "    - To get final inverse, keep passing x through inverse methods of layers\n",
    "    - I didn't implement batch norm as I didn't want to implement an inverse batch norm\n",
    "    '''\n",
    "    def __init__(self, dim, hidden_dim, model_name, num_layers=5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        for _ in range(num_layers):\n",
    "            if model_name == 'maf':\n",
    "                model_instance = MAF(dim, hidden_dim, reverse_order=True)\n",
    "            elif model_name == 'iaf':\n",
    "                model_instance = IAF(dim, hidden_dim, reverse_order=True)\n",
    "            elif model_name == 'nice':\n",
    "                model_instance = NICE(dim, hidden_dim, reverse_order=True)\n",
    "                \n",
    "            self.layers.append(model_instance)\n",
    "\n",
    "        \n",
    "        if model_name == 'nice':\n",
    "            self.layers.append(DiagonalScalingLayer(dim))\n",
    "        \n",
    "    def forward(self, z):\n",
    "        ''' f: Z -> X = SAMPLING\n",
    "        '''\n",
    "        log_det_total = torch.zeros(z.size(0), device=device) # [BS,]\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            z, log_det = self.layers[i](z) # output of one layer passed into next\n",
    "            # flipping handled internally after each layer\n",
    "            log_det_total += log_det # final log_det of the entire transformation is sum of log_dets\n",
    "        \n",
    "        # this final z after being transformed through all the layers = x\n",
    "        return z, log_det_total # [BS, D], [BS,]\n",
    "        \n",
    "    def inverse(self, x):\n",
    "        ''' f: X -> Z = TRAINING\n",
    "        '''\n",
    "        log_det_total = torch.zeros(x.size(0), device=device) # [BS,]\n",
    "        \n",
    "        for i in reversed(range(self.num_layers)):\n",
    "            x, log_det = self.layers[i].inverse(x)\n",
    "            log_det_total += log_det\n",
    "        \n",
    "        # this final x after being transformed through all the layers = z\n",
    "        return x, log_det_total # [BS, D], [BS,]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a0913c",
   "metadata": {},
   "source": [
    "## Training & Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a8fd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_samples(model, input_dim):\n",
    "    model.eval()\n",
    "    \n",
    "    z = torch.randn((batch_size, input_dim), device=device) # [BS, D]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        x, _ = model(z) # [BS, D]\n",
    "    \n",
    "    x_images = x.view(-1, int(np.sqrt(input_dim)), int(np.sqrt(input_dim))).cpu().numpy()\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 5, figsize=(10, 4))  # Adjust figsize as needed\n",
    "    for i, ax in enumerate(axes.flatten()):\n",
    "        if i < 10:  # Make sure to only access the first 10 images\n",
    "            ax.imshow(x_images[i], cmap='gray')\n",
    "        ax.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed10324",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNormFlow(model, train_loader, optimizer, num_epochs, input_dim):\n",
    "    pz = MultivariateNormal(\n",
    "        torch.zeros(input_dim, device=device), \n",
    "        torch.eye(input_dim, device=device)\n",
    "    )\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for i, batch in enumerate(train_loader):\n",
    "            x, _ = batch # [BS, D]\n",
    "            x = x.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # MAF inverse: X -> Z\n",
    "            # p_X(x) = pZ(f^-1(x)) * |det(df^-1/dx)| = pZ(f^-1(x)) + exp(-alpha summed over i)\n",
    "            # log(p_X(x)) = log(pZ(f^-1(x))) + log(exp(-alpha)) = log(pZ(z)) + (-alpha summed over i)\n",
    "            z, log_det = model.inverse(x) # [BS, D], [BS,]\n",
    "            log_pz = pz.log_prob(z) # log_pz = [BS,]\n",
    "            # log_pz = -0.5 * input_dim * torch.log(torch.tensor(2 * math.pi)) - 0.5 * (z ** 2).sum(dim=1)  # [BS,]\n",
    "            \n",
    "            # minimizing KL divergence between p_X and p_data = argmin_pX KL(p_data||p_X)\n",
    "            # argmin_pX KL(p_data||p_X) = argmin_pX -\\Sigma_{x} p_data(x) * log(p_X(x))\n",
    "            # = argmin_pX -log(p_X(x)) over the real data, x\n",
    "            log_px = log_pz + log_det # [BS,] + [BS,]\n",
    "            loss = -log_px.mean() # averaged over the batch\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Loss: {train_loss / len(train_loader)}\")\n",
    "        if epoch != 0 and epoch % 10 == 0:\n",
    "            view_samples(model, input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c3f7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 64\n",
    "hidden_dim = 128\n",
    "num_layers = 5\n",
    "\n",
    "models_q1 = {'maf':None, 'nice':None}\n",
    "\n",
    "for model_name in models_q1.keys():\n",
    "    model = NormFlow(input_dim, hidden_dim, model_name, num_layers)\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    trainNormFlow(model, train_loader, optimizer, num_epochs, input_dim)\n",
    "    \n",
    "    models_q1[model_name] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f378f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in models_q1.items():\n",
    "    print(f'Results after {num_epochs} epochs of training for {model_name}')\n",
    "    view_samples(model, input_dim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
